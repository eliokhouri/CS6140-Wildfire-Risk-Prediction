{"cells":[{"cell_type":"markdown","source":["# Scripts to mine, process and convert the LCD station data\n","\n","\n","> (Worked on this locally, hence the local paths.)\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"nhy1KyMEhOvu"},"id":"nhy1KyMEhOvu"},{"cell_type":"markdown","source":["## After the LCD files were downloaded, we renamed the files, did some manipulations and some web page scraping to get additional data."],"metadata":{"id":"fyEq9-Zv0Wnh"},"id":"fyEq9-Zv0Wnh"},{"cell_type":"code","execution_count":null,"id":"dc541541","metadata":{"id":"dc541541","outputId":"60213200-7a8a-4053-df2e-10eb8e0b6d20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in /Users/Elliot/opt/anaconda3/lib/python3.9/site-packages (1.4.4)\n","Requirement already satisfied: openpyxl in /Users/Elliot/opt/anaconda3/lib/python3.9/site-packages (3.0.10)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /Users/Elliot/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /Users/Elliot/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2022.1)\n","Requirement already satisfied: numpy>=1.20.0 in /Users/Elliot/opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n","Requirement already satisfied: et_xmlfile in /Users/Elliot/opt/anaconda3/lib/python3.9/site-packages (from openpyxl) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /Users/Elliot/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pandas openpyxl"]},{"cell_type":"code","execution_count":null,"id":"833f709c","metadata":{"id":"833f709c"},"outputs":[],"source":["import os\n","import csv\n","\n","####################################################################################\n","# Renames the csvs to the station ids\n","####################################################################################\n","\n","# Define the directory where your CSV files are located\n","csv_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download'\n","\n","# List all CSV files in the directory\n","csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n","\n","# Loop through each CSV file\n","for csv_file in csv_files:\n","    csv_path = os.path.join(csv_directory, csv_file)\n","\n","    # Open the CSV file\n","    with open(csv_path, 'r') as file:\n","        # Read the first row (header) to determine the column names\n","        csv_reader = csv.reader(file)\n","        header = next(csv_reader)\n","\n","        # Find the index of the \"STATION\" column\n","        station_column_index = header.index(\"STATION\")\n","\n","        # Read the last 5 characters from the \"STATION\" column value\n","        id_value = None\n","        for row in csv_reader:\n","            station_value = row[station_column_index]\n","            id_value = station_value[-5:]  # Extract the last 5 characters\n","            break  # We only need the first row\n","\n","    if id_value:\n","        # Generate the new file name based on the extracted ID value\n","        new_file_name = f'{id_value}.csv'\n","\n","        # Rename the CSV file\n","        new_csv_path = os.path.join(csv_directory, new_file_name)\n","        os.rename(csv_path, new_csv_path)\n","\n","        print(f'Renamed {csv_file} to {new_file_name}')\n","    else:\n","        print(f'Unable to extract ID from {csv_file}')\n"]},{"cell_type":"code","execution_count":null,"id":"f54a8587","metadata":{"id":"f54a8587"},"outputs":[],"source":["import pandas as pd\n","\n","####################################################################################\n","# gets the difference between the 151 all station and my station count\n","####################################################################################\n","\n","\n","def compare_station_ids(csv_all_stations, csv_my_stations, output_csv):\n","    df_all = pd.read_csv(csv_all_stations, dtype={'STATION_ID': str})\n","    df_my = pd.read_csv(csv_my_stations, dtype={'STATION_ID': str})\n","\n","    set_all = set(df_all['STATION_ID'])\n","    set_my = set(df_my['STATION_ID'])\n","\n","    # Find station IDs that are different\n","    diff_all = set_all - set_my\n","    diff_my = set_my - set_all\n","\n","    # Extract rows for differing station IDs from both DataFrames\n","    diff_all_df = df_all[df_all['STATION_ID'].isin(diff_all)]\n","    diff_my_df = df_my[df_my['STATION_ID'].isin(diff_my)]\n","\n","    # Combine the two DataFrames\n","    diff_df = pd.concat([diff_all_df, diff_my_df], ignore_index=True)\n","\n","    # Save the differences to a CSV file\n","    diff_df.to_csv(output_csv, index=False)\n","    print(f\"Differences saved to {output_csv}.\")\n","\n","# Paths to your CSV files\n","csv_all_stations = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/LCD_ALL_STATIONS_ALL_INFO.csv'\n","csv_my_stations = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/LCD_MY_STATIONS_ALL_INFO.csv'\n","output_csv = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/LCD_STATION_DIFFERENCES.csv'\n","\n","# Compare the station IDs and output the differences\n","compare_station_ids(csv_all_stations, csv_my_stations, output_csv)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b7381cda","metadata":{"scrolled":true,"id":"b7381cda"},"outputs":[],"source":["import pandas as pd\n","\n","####################################################################################\n","# gets the list of station ids with leading zeros intact.\n","####################################################################################\n","\n","# Replace the path with your actual file path\n","excel_file_path = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/LCD_ALL_STATIONS.xlsx'\n","\n","# Read the Excel file\n","df = pd.read_excel(excel_file_path)\n","\n","# Assuming 'STATION_ID' is the column name with the station IDs\n","station_ids = df['STATION_ID'].tolist()\n","\n","# Convert the list into a string format with leading zeros intact, ready to be pasted into the scraping script\n","station_ids_string = ', '.join([f\"'{str(id).zfill(5)}'\" for id in station_ids])\n","\n","# Print the formatted list of station IDs\n","print(f\"station_ids = [{station_ids_string}]\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3866e2b0","metadata":{"id":"3866e2b0"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","\n","####################################################################################\n","#scrapes all 151 webpages to create a comprehensive file of all stations with their lat, long, name and elavation\n","####################################################################################\n","\n","# List of station IDs as strings to maintain leading zeros\n","station_ids = [\n","    '23239', '94299', '24283', '23224', '23191', '23155', '23161', '93216',\n","    '03182', '00115', '23157', '23225', '23158', '93245', '00433', '23152',\n","    '23136', '03154', '03164', '03177', '23203', '93203', '93104', '03179',\n","    '00206', '23254', '53175', '24286', '23240', '53186', '03104', '53144',\n","    '23114', '23199', '03165', '93101', '24213', '53151', '23167', '93193',\n","    '93217', '03166', '53143', '00135', '00228', '53119', '03167', '93228',\n","    '00392', '93115', '03144', '93194', '03180', '03159', '23110', '00205',\n","    '23285', '23243', '23129', '00117', '53141', '93134', '23174', '53130',\n","    '93242', '03181', '23119', '93205', '00369', '93243', '23257', '23258',\n","    '23244', '03183', '24259', '23259', '24215', '93136', '93227', '23179',\n","    '00397', '93112', '23230', '53121', '03102', '93210', '93110', '93138',\n","    '23182', '23289', '93209', '00320', '00227', '93111', '23149', '53120',\n","    '24216', '04222', '24257', '03171', '00396', '23271', '23232', '23206',\n","    '23208', '93225', '23233', '23122', '93231', '93117', '03178', '23188',\n","    '93107', '03131', '23272', '23234', '93232', '23293', '93206', '93116',\n","    '93226', '23187', '93184', '53152', '23190', '23273', '93197', '23213',\n","    '93244', '04204', '00395', '93230', '23237', '53139', '00479', '03122',\n","    '03174', '23202', '00346', '93201', '93114', '93121', '23275', '00174',\n","    '93241', '23130', '93214', '23131', '93144', '23277', '53150'\n","]\n","\n","# Initialize your dataframe with the updated column names\n","df = pd.DataFrame(columns=['STATION_ID', 'STATION_NAME', 'STATION_LATITUDE', 'STATION_LONGITUDE', 'STATION_ELEVATION'])\n","\n","# Define the base URL\n","base_url = \"https://www.ncei.noaa.gov/cdo-web/datasets/LCD/stations/WBAN:\"\n","\n","# Function to scrape latitude, longitude, elevation, and station name\n","def scrape_station_data(station_id):\n","    # Construct the URL for the current station ID\n","    url = f\"{base_url}{station_id}/detail\"\n","\n","    # Send a request to the URL\n","    response = requests.get(url)\n","\n","    # If the request was successful, proceed to parse the data\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # Find the 'Name' row and extract the station name\n","        name_td = soup.find('td', text='Name')\n","        station_name = name_td.find_next_sibling('td').text if name_td else ''\n","\n","        # Find the 'Latitude/Longitude' row and extract the values\n","        lat_lon_td = soup.find('td', text='Latitude/Longitude')\n","        lat, lon = ('', '')\n","        if lat_lon_td:\n","            lat_lon_val_td = lat_lon_td.find_next_sibling('td')\n","            if lat_lon_val_td:\n","                lat_lon_text = lat_lon_val_td.text.strip()\n","                lat, lon = lat_lon_text.replace('°', '').split(',')\n","\n","        # Find the 'Elevation' row and extract the value\n","        elevation_td = soup.find('td', text='Elevation')\n","        elevation = elevation_td.find_next_sibling('td').text.strip() if elevation_td else ''\n","\n","        return station_name.strip(), lat.strip(), lon.strip(), elevation.strip()\n","    else:\n","        return '', '', '', ''\n","\n","# Function to print messages in color\n","def print_success(message):\n","    print(f\"\\033[92m{message}\\033[0m\")  # Green text\n","\n","def print_failure(message):\n","    print(f\"\\033[91m{message}\\033[0m\")  # Red text\n","\n","# Iterate over each station ID and scrape the data\n","results = []\n","for station_id in station_ids:\n","    station_name, lat, lon, elevation = scrape_station_data(station_id)\n","    if station_name and lat and lon and elevation:\n","        results.append({\n","            'STATION_ID': station_id,\n","            'STATION_NAME': station_name,\n","            'STATION_LATITUDE': lat,\n","            'STATION_LONGITUDE': lon,\n","            'STATION_ELEVATION': elevation\n","        })\n","        print_success(f\"{station_id}: SUCCESS\")\n","    else:\n","        print_failure(f\"{station_id}: FAIL\")\n","\n","# Convert the list of dictionaries to a DataFrame and concat with the main DataFrame\n","df = pd.concat([df, pd.DataFrame(results)], ignore_index=True)\n","\n","# Define a valid path for your system\n","output_path = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/LCD_ALL_STATIONS_ALL_INFO.csv'\n","\n","# Ensure the directory exists\n","os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","# Save the DataFrame to a CSV file\n","print(\"Scraping complete. Saving to CSV...\")\n","df.to_csv(output_path, index=False)\n","print(f\"CSV file has been saved to {output_path}.\")\n"]},{"cell_type":"code","execution_count":null,"id":"6ad0a8e2","metadata":{"id":"6ad0a8e2"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","\n","####################################################################################\n","# gets all the files i downloaded and uses them to determine\n","# which webpages to scrape to create a comprehensive file of all MY stations with their lat, long, name and elavation\n","####################################################################################\n","\n","# Function to get station IDs from file names in the given directory\n","def get_station_ids_from_files(directory_path):\n","    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n","    station_ids = [f.split('.')[0] for f in csv_files]\n","    return station_ids\n","\n","# Function to scrape latitude, longitude, elevation, and station name\n","def scrape_station_data(station_id):\n","    base_url = \"https://www.ncei.noaa.gov/cdo-web/datasets/LCD/stations/WBAN:\"\n","    url = f\"{base_url}{station_id}/detail\"\n","\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()  # This will raise an HTTPError if the HTTP request returned an unsuccessful status code\n","\n","    except requests.exceptions.HTTPError as errh:\n","        print(\"Http Error:\", errh)\n","        return '', '', '', ''\n","    except requests.exceptions.ConnectionError as errc:\n","        print(\"Error Connecting:\", errc)\n","        return '', '', '', ''\n","    except requests.exceptions.Timeout as errt:\n","        print(\"Timeout Error:\", errt)\n","        return '', '', '', ''\n","    except requests.exceptions.RequestException as err:\n","        print(\"Oops: Something Else\", err)\n","        return '', '', '', ''\n","\n","    try:\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # Extract station name\n","        name_td = soup.find('td', text='Name')\n","        station_name = name_td.find_next_sibling('td').text.strip() if name_td else ''\n","\n","        # Extract latitude and longitude\n","        lat_lon_td = soup.find('td', text='Latitude/Longitude')\n","        lat, lon = ('', '')\n","        if lat_lon_td:\n","            lat_lon_val_td = lat_lon_td.find_next_sibling('td')\n","            if lat_lon_val_td:\n","                lat_lon_text = lat_lon_val_td.text.strip()\n","                lat, lon = lat_lon_text.replace('°', '').split(',')\n","                # Clean up any extra whitespace or characters\n","                lat = lat.strip()\n","                lon = lon.strip().replace('W', '-').replace('E', '')  # Assuming Western Hemisphere for 'W'\n","\n","        # Extract elevation\n","        elevation_td = soup.find('td', text='Elevation')\n","        elevation = elevation_td.find_next_sibling('td').text.strip() if elevation_td else ''\n","\n","        return station_name, lat, lon, elevation\n","\n","    except Exception as e:\n","        print(f\"An error occurred while parsing data for station {station_id}: {e}\")\n","        return '', '', '', ''\n","\n","# Function to print messages in color\n","def print_success(message):\n","    print(f\"\\033[92m{message}\\033[0m\")  # Green text\n","\n","def print_failure(message):\n","    print(f\"\\033[91m{message}\\033[0m\")  # Red text\n","\n","# Path to the directory containing the CSV files\n","directory_path = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download'\n","\n","# Use the function to dynamically create a list of station IDs from CSV filenames in the directory\n","station_ids = get_station_ids_from_files(directory_path)\n","\n","# Initialize dataframe with updated column names\n","df = pd.DataFrame(columns=['STATION_ID', 'STATION_NAME', 'STATION_LATITUDE', 'STATION_LONGITUDE', 'STATION_ELEVATION'])\n","\n","# Iterate over each station ID and scrape the data\n","results = []\n","for station_id in station_ids:\n","    station_name, lat, lon, elevation = scrape_station_data(station_id)\n","    if station_name and lat and lon and elevation:\n","        results.append({\n","            'STATION_ID': station_id,\n","            'STATION_NAME': station_name,\n","            'STATION_LATITUDE': lat,\n","            'STATION_LONGITUDE': lon,\n","            'STATION_ELEVATION': elevation\n","        })\n","        print_success(f\"{station_id}: SUCCESS\")\n","    else:\n","        print_failure(f\"{station_id}: FAIL\")\n","\n","# Convert the list of dictionaries to a DataFrame and concatenate with the main DataFrame\n","df = pd.concat([df, pd.DataFrame(results)], ignore_index=True)\n","\n","# Define a valid path for your system\n","output_path = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/LCD_MY_STATIONS_ALL_INFO.csv'\n","\n","# Ensure the directory exists\n","os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","# Save the DataFrame to a CSV file\n","print(\"Scraping complete. Saving to CSV...\")\n","df.to_csv(output_path, index=False)\n","print(f\"CSV file has been saved to {output_path}.\")"]},{"cell_type":"markdown","id":"fc4c4c17","metadata":{"id":"fc4c4c17"},"source":["## We begin processing and combining the raw data.\n","\n","\n","> In this section I combined all the data into one file, I would then modify this approach to keep the station data files seperate. The section after this is where I take that alternative approach.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"91e1a7c4","metadata":{"id":"91e1a7c4","outputId":"29b34996-b245-4f46-b951-189e14dfe039"},"outputs":[{"name":"stdout","output_type":"stream","text":["All files have the same structure.\n"]}],"source":["import os\n","import pandas as pd\n","import glob\n","\n","####################################################################################\n","# Check if all files have the same structure.\n","####################################################################################\n","\n","\n","# Directory containing individual CSV files\n","input_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download'\n","\n","# List of all CSV files in the input directory\n","csv_files = glob.glob(os.path.join(input_directory, '*.csv'))\n","\n","# Initialize a dictionary to store the column information of each CSV\n","csv_structure = {}\n","\n","# Go through each file and check the structure\n","for file in csv_files:\n","    try:\n","        # Read just the first row to get the column headers\n","        df = pd.read_csv(file, nrows=1)\n","        # Create a tuple of the column headers\n","        columns = tuple(df.columns.tolist())\n","        # Add the column structure to the dictionary with the filename as key\n","        csv_structure[file] = columns\n","    except Exception as e:\n","        print(f\"Error reading {file}: {e}\")\n","\n","# Check if all files have the same structure\n","first_file_columns = csv_structure[list(csv_structure.keys())[0]]\n","consistent_structure = all(columns == first_file_columns for columns in csv_structure.values())\n","\n","# Print the results\n","if consistent_structure:\n","    print(\"All files have the same structure.\")\n","else:\n","    print(\"Files have inconsistent structures. Please review the differences.\")\n","    for file, columns in csv_structure.items():\n","        if columns != first_file_columns:\n","            print(f\"File {file} has a different structure.\")\n"]},{"cell_type":"code","execution_count":null,"id":"357f6990","metadata":{"id":"357f6990"},"outputs":[],"source":["# import os\n","# import pandas as pd\n","# import glob\n","\n","# ####################################################################################\n","# # Combine into 1 file.\n","# ####################################################################################\n","\n","\n","# # Directory containing individual CSV files\n","# input_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download'\n","\n","# # Output directory for the combined CSV\n","# output_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA'\n","# output_filename = 'ALL_LCD_RAW_DATA_COMBINED.csv'\n","# output_filepath = os.path.join(output_directory, output_filename)\n","\n","# # List of all CSV files in the input directory\n","# csv_files = glob.glob(os.path.join(input_directory, '*.csv'))\n","# print(f\"Found {len(csv_files)} files to process.\")\n","\n","# # Initialize a list to hold dataframes\n","# dataframes = []\n","\n","# # Loop through each file, read it into a pandas DataFrame, and add it to the list\n","# for i, file in enumerate(csv_files, 1):\n","#     try:\n","#         print(f\"Processing file {i}/{len(csv_files)}: {file}\")\n","#         df = pd.read_csv(file, low_memory=False)\n","#         dataframes.append(df)\n","#     except Exception as e:\n","#         print(f\"Error processing {file}: {e}\")\n","\n","# # Combine all files in the list\n","# print(\"Combining files...\")\n","# combined_csv = pd.concat(dataframes)\n","\n","# # Export to CSV\n","# print(f\"Writing combined data to {output_filepath}...\")\n","# combined_csv.to_csv(output_filepath, index=False)\n","\n","# print(\"All files have been combined and saved successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b0781e55","metadata":{"id":"b0781e55"},"outputs":[],"source":["# import os\n","# import pandas as pd\n","# import glob\n","\n","# ####################################################################################\n","# # Check the length of the file and the individual files.\n","# ####################################################################################\n","\n","\n","# # Directory containing individual CSV files\n","# input_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download'\n","\n","# # Path to the combined CSV file\n","# combined_csv_path = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/ALL_LCD_RAW_DATA_COMBINED.csv'\n","\n","# # List of all CSV files in the input directory\n","# csv_files = glob.glob(os.path.join(input_directory, '*.csv'))\n","\n","# # Calculate the sum of rows of all individual CSV files\n","# # Subtract 1 for the header row in each file\n","# total_rows_individual_files = sum(pd.read_csv(file, dtype=str, usecols=[0]).shape[0] for file in csv_files) - len(csv_files)\n","\n","# # Calculate the number of rows in the combined CSV file\n","# # Here we assume the combined CSV also has a single header row\n","# combined_csv_rows = pd.read_csv(combined_csv_path, dtype=str, usecols=[0]).shape[0] - 1\n","\n","# # Compare the numbers\n","# if total_rows_individual_files == combined_csv_rows:\n","#     print(\"The row counts match. The files have been combined successfully.\")\n","# else:\n","#     print(\"The row counts do not match.\")\n","#     print(f\"Total rows in individual files (excluding headers): {total_rows_individual_files}\")\n","#     print(f\"Total rows in combined file (excluding header): {combined_csv_rows}\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4c6d28d2","metadata":{"id":"4c6d28d2","outputId":"19c27a62-a415-4ea9-ce38-9645e6345885"},"outputs":[{"name":"stdout","output_type":"stream","text":["The modified file has been saved to /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/MODIFIED_LCD_RAW_DATA.csv\n"]}],"source":["# import pandas as pd\n","\n","# ####################################################################################\n","# # Remove extra columns from combined file and reformate date and the station id.\n","# ####################################################################################\n","\n","# # Path to the combined CSV file\n","# combined_csv_path = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/ALL_LCD_RAW_DATA_COMBINED.csv'\n","\n","# # New file path for the modified CSV file\n","# modified_csv_path = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/MODIFIED_LCD_RAW_DATA.csv'\n","\n","\n","# # Columns you want to keep\n","# columns_to_keep = [\n","#     'STATION', 'DATE', 'HourlyDewPointTemperature', 'HourlyDryBulbTemperature',\n","#     'HourlyPrecipitation', 'HourlyPresentWeatherType', 'HourlyRelativeHumidity',\n","#     'HourlySkyConditions', 'HourlyWetBulbTemperature', 'HourlyWindDirection',\n","#     'HourlyWindGustSpeed', 'HourlyWindSpeed'\n","# ]\n","\n","# # Specify dtype as string for all columns to avoid DtypeWarning\n","# dtype_dict = {col: str for col in columns_to_keep}\n","\n","# # Read the combined CSV file, selecting only the columns to keep, with specified data types\n","# df = pd.read_csv(combined_csv_path, usecols=columns_to_keep, dtype=dtype_dict)\n","\n","# # Convert the 'DATE' column to datetime\n","# df['DATE'] = pd.to_datetime(df['DATE'])\n","\n","# # Extract year, month, day, hour, and minute into separate columns\n","# df['YEAR'] = df['DATE'].dt.year\n","# df['MONTH'] = df['DATE'].dt.month\n","# df['DAY'] = df['DATE'].dt.day\n","# df['HOUR'] = df['DATE'].dt.hour  # Extract hour\n","# df['MINUTE'] = df['DATE'].dt.minute  # Extract minute\n","\n","# # Extract the last 5 digits of the 'STATION' column to a new 'STATION_ID' column\n","# df['STATION_ID'] = df['STATION'].str[-5:]\n","\n","# # Drop the original 'STATION' column if you no longer need it\n","# # df = df.drop('STATION', axis=1)\n","\n","# # Reorder columns to have the date and time components next to each other, if desired\n","# df = df[['STATION_ID', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'HourlyDewPointTemperature',\n","#          'HourlyDryBulbTemperature', 'HourlyPrecipitation', 'HourlyPresentWeatherType',\n","#          'HourlyRelativeHumidity', 'HourlySkyConditions', 'HourlyWetBulbTemperature',\n","#          'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed']]\n","\n","# # Write the modified DataFrame to a new CSV file\n","# df.to_csv(modified_csv_path, index=False)\n","\n","# print(f\"The modified file has been saved to {modified_csv_path}\")\n"]},{"cell_type":"code","execution_count":null,"id":"cde6e508","metadata":{"id":"cde6e508","outputId":"316b4539-92c0-449a-9548-26e101a32d09"},"outputs":[{"name":"stderr","output_type":"stream","text":["Reading first file: 217it [02:06,  1.71it/s]\n","Reading second file: 217it [00:11, 18.84it/s]"]},{"name":"stdout","output_type":"stream","text":["Number of rows in the first file: 21632463\n","Number of rows in the second file: 21632463\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# import pandas as pd\n","# from tqdm import tqdm\n","\n","# ####################################################################################\n","# # Checking file length.\n","# ####################################################################################\n","\n","\n","# # File paths\n","# file_path_1 = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/ALL_LCD_RAW_DATA_COMBINED.csv'\n","# file_path_2 = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/MODIFIED_LCD_RAW_DATA.csv'\n","\n","# # Define a chunk size\n","# chunk_size = 100000  # This means the CSV will be processed in chunks of 100,000 rows\n","\n","# # Initialize a progress bar for each file without a total since we don't know the total number of rows\n","# chunks1 = pd.read_csv(file_path_1, chunksize=chunk_size, low_memory=False)\n","# chunks2 = pd.read_csv(file_path_2, chunksize=chunk_size, low_memory=False)\n","\n","# # Process the first file with a progress bar\n","# num_rows_df1 = 0\n","# for chunk in tqdm(chunks1, desc='Reading first file'):\n","#     num_rows_df1 += chunk.shape[0]\n","\n","# # Process the second file with a progress bar\n","# num_rows_df2 = 0\n","# for chunk in tqdm(chunks2, desc='Reading second file'):\n","#     num_rows_df2 += chunk.shape[0]\n","\n","# # Print the number of rows\n","# print(f\"Number of rows in the first file: {num_rows_df1}\")\n","# print(f\"Number of rows in the second file: {num_rows_df2}\")\n"]},{"cell_type":"markdown","id":"824b0ca3","metadata":{"id":"824b0ca3"},"source":["## Below we are working on modifying the files but without combining them.\n","> Originally we combined them into one file but we modified the approach to have each station's data contained in its own file and store them all in a folder.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"813c5f79","metadata":{"id":"813c5f79","outputId":"d211f9dc-d29e-48a4-a3ce-0176feb35518"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23131.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23131.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23119.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23119.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03183.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03183.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03154.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03154.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93209.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93209.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23285.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23285.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03182.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03182.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23130.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23130.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93197.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93197.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/24259.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/24259.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03180.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03180.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23244.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23244.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23293.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23293.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03181.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03181.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00117.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00117.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93227.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93227.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93232.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93232.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23254.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23254.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93144.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93144.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23136.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23136.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93193.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93193.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23122.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23122.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03179.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03179.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93230.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93230.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93231.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93231.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03144.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03144.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93225.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93225.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03178.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03178.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23257.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23257.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23243.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23243.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00115.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00115.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93184.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93184.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53175.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53175.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23152.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23152.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93134.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93134.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23191.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23191.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23224.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23224.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23230.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23230.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93242.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93242.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93243.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93243.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23225.csv\n"]},{"name":"stdout","output_type":"stream","text":["File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23225.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93121.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93121.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23190.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23190.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00205.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00205.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53139.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53139.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23179.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23179.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23233.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23233.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93241.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93241.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/24213.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/24213.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23232.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23232.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23187.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23187.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00206.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00206.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00174.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00174.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03131.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03131.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93244.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93244.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93245.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93245.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23237.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23237.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/24216.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/24216.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23182.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23182.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23155.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23155.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23157.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23157.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/94299.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/94299.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23234.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23234.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23208.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23208.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/24215.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/24215.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00228.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00228.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93115.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93115.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00392.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00392.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03102.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03102.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23199.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23199.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53130.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53130.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23158.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23158.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93116.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93116.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23206.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23206.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00346.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00346.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23213.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23213.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93117.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93117.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00227.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00227.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53119.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53119.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53121.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53121.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23149.csv\n"]},{"name":"stdout","output_type":"stream","text":["File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23149.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23161.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23161.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93107.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93107.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23203.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23203.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03104.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03104.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23202.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23202.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00395.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00395.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93112.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93112.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23174.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23174.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53120.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53120.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93138.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93138.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93110.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93110.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93104.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93104.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00397.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00397.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00369.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00369.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00433.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00433.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00396.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00396.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93111.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93111.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23188.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23188.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53150.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53150.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53144.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53144.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23110.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23110.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/04222.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/04222.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23272.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23272.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93214.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93214.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93228.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93228.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93201.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93201.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03174.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03174.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23273.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23273.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53151.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53151.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53186.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53186.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23271.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23271.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23259.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23259.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93203.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93203.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/24286.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/24286.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03177.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03177.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93216.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93216.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23258.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23258.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53152.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53152.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00320.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00320.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03167.csv\n"]},{"name":"stdout","output_type":"stream","text":["File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03167.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93206.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93206.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03166.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03166.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/24283.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/24283.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23275.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23275.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53143.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53143.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/53141.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/53141.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23129.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23129.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00135.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00135.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23277.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23277.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93205.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93205.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/00479.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/00479.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03165.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03165.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/93210.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/93210.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03171.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03171.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/03159.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/03159.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23289.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23289.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/24257.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/24257.csv\n","Processing file: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download/23114.csv\n","File saved: /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod/23114.csv\n","Column 'STATION' has data types: {'object', 'int64'}\n","Column 'DATE' has data types: {'object'}\n","Column 'HourlyDewPointTemperature' has data types: {'object', 'float64'}\n","Column 'HourlyDryBulbTemperature' has data types: {'object', 'float64'}\n","Column 'HourlyPrecipitation' has data types: {'object', 'float64'}\n","Column 'HourlyPresentWeatherType' has data types: {'object', 'float64'}\n","Column 'HourlyRelativeHumidity' has data types: {'object', 'float64'}\n","Column 'HourlySkyConditions' has data types: {'object', 'float64'}\n","Column 'HourlyWetBulbTemperature' has data types: {'object', 'float64'}\n","Column 'HourlyWindDirection' has data types: {'object', 'float64'}\n","Column 'HourlyWindGustSpeed' has data types: {'object', 'float64'}\n","Column 'HourlyWindSpeed' has data types: {'object', 'float64'}\n"]}],"source":["import pandas as pd\n","import os\n","import glob\n","\n","####################################################################################\n","# Drop the extra columns from the 139 files and from the ones we keep get a tally of the data types\n","# they have. Move the modified files withot the extra columns to a new folder.\n","# Print the types of data in the columns.\n","####################################################################################\n","\n","\"\"\"\n","Column 'STATION' data type examples:\n","  int: [74718703104.0, 72064500227.0, 74509023244.0, 72287493134.0, 72295303183.0]\n","  float: []\n","  str: ['A0705300346', 'A0704900320', 'A0685400115']\n","\n","Column 'DATE' data type examples:\n","  int: []\n","  float: []\n","  str: ['2014-08-21T00:53:00', '2014-11-06T09:56:00', '2014-11-10T01:35:00', '2014-11-06T09:55:00', '2014-11-06T01:58:00']\n","\"\"\"\n","\n","# Input and output directories\n","input_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download'\n","output_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod'\n","\n","# Make sure the output directory exists\n","os.makedirs(output_directory, exist_ok=True)\n","\n","# List of all CSV files in the input directory\n","csv_files = glob.glob(os.path.join(input_directory, '*.csv'))\n","\n","# Columns to keep\n","columns_to_keep = [\n","    'STATION', 'DATE', 'HourlyDewPointTemperature', 'HourlyDryBulbTemperature',\n","    'HourlyPrecipitation', 'HourlyPresentWeatherType', 'HourlyRelativeHumidity',\n","    'HourlySkyConditions', 'HourlyWetBulbTemperature', 'HourlyWindDirection',\n","    'HourlyWindGustSpeed', 'HourlyWindSpeed'\n","]\n","\n","# Initialize a dictionary to store column data types across files\n","column_data_types = {col: set() for col in columns_to_keep}\n","\n","# Process each file\n","for file in csv_files:\n","    try:\n","        print(f\"Processing file: {file}\")\n","\n","        # Read file with specified columns\n","        df = pd.read_csv(file, usecols=columns_to_keep, low_memory=False)\n","\n","        # Check and record the data types of each column\n","        for col in df.columns:\n","            column_data_types[col].add(str(df[col].dtype))\n","\n","        # Save to output directory\n","        output_file = os.path.join(output_directory, os.path.basename(file))\n","        df.to_csv(output_file, index=False)\n","\n","        print(f\"File saved: {output_file}\")\n","\n","    except Exception as e:\n","        print(f\"Error processing {file}: {e}\")\n","\n","# Display the data types found for each column across all files\n","for col, dtypes in column_data_types.items():\n","    print(f\"Column '{col}' has data types: {dtypes}\")\n"]},{"cell_type":"code","execution_count":null,"id":"4c2cb251","metadata":{"id":"4c2cb251","outputId":"a39e53db-fe3d-4e6b-d673-7ca21e597993"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing complete.\n"]}],"source":["import pandas as pd\n","import os\n","import glob\n","\n","\n","####################################################################################\n","# Folder containing the 139 relevant stations is further modified.\n","# Now they contain only the relevant features, and the date column is split down into more granular columns\n","# (year, month, day, hour, minute). Stripping the last 5 digits from the station column to\n","# get the station id as well.\n","####################################################################################\n","\n","\n","# Directory containing the CSV files to be modified\n","input_directory = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod'\n","\n","# List of all CSV files in the input directory\n","csv_files = glob.glob(os.path.join(input_directory, '*.csv'))\n","\n","# Process each file\n","for file in csv_files:\n","    try:\n","        # Read the file\n","        df = pd.read_csv(file, dtype=str)\n","\n","        # Extract the last 5 characters from 'STATION' column for 'STATION_ID'\n","        df['STATION_ID'] = df['STATION'].str[-5:].str.zfill(5)\n","\n","        # Convert 'DATE' column to datetime and extract components\n","        df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n","        df['YEAR'] = df['DATE'].dt.year\n","        df['MONTH'] = df['DATE'].dt.month\n","        df['DAY'] = df['DATE'].dt.day\n","        df['HOUR'] = df['DATE'].dt.hour\n","        df['MINUTE'] = df['DATE'].dt.minute\n","\n","        # Reorder and select columns as specified\n","        df = df[['STATION_ID', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'HourlyDewPointTemperature',\n","                 'HourlyDryBulbTemperature', 'HourlyPrecipitation', 'HourlyPresentWeatherType',\n","                 'HourlyRelativeHumidity', 'HourlySkyConditions', 'HourlyWetBulbTemperature',\n","                 'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed']]\n","\n","        # Save the modified DataFrame back to the same file\n","        df.to_csv(file, index=False)\n","\n","    except Exception as e:\n","        print(f\"Error processing {file}: {e}\")\n","\n","print(\"Processing complete.\")\n"]},{"cell_type":"markdown","id":"d5640997","metadata":{"id":"d5640997"},"source":["## Need to go back and convert LCD into daily measurements.\n","\n","\n","> After we had the LCD files we realized we need to go back and convert the LCD files to daily measurements to match GHCN and the other datasets.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d9fd3ee5","metadata":{"id":"d9fd3ee5","outputId":"f6c587b8-90b8-4d36-8812-51246c53edfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preprocessed and saved data for 23131.csv\n","Preprocessed and saved data for 23119.csv\n","Preprocessed and saved data for 03183.csv\n","Preprocessed and saved data for 03154.csv\n","Preprocessed and saved data for 93209.csv\n","Preprocessed and saved data for 23285.csv\n","Preprocessed and saved data for 03182.csv\n","Preprocessed and saved data for 23130.csv\n","Preprocessed and saved data for 93197.csv\n","Preprocessed and saved data for 24259.csv\n","Preprocessed and saved data for 03180.csv\n","Preprocessed and saved data for 23244.csv\n","Preprocessed and saved data for 23293.csv\n","Preprocessed and saved data for 03181.csv\n","Preprocessed and saved data for 00117.csv\n","Preprocessed and saved data for 93227.csv\n","Preprocessed and saved data for 93232.csv\n","Preprocessed and saved data for 23254.csv\n","Preprocessed and saved data for 93144.csv\n","Preprocessed and saved data for 23136.csv\n","Preprocessed and saved data for 93193.csv\n","Preprocessed and saved data for 23122.csv\n","Preprocessed and saved data for 03179.csv\n","Preprocessed and saved data for 93230.csv\n","Preprocessed and saved data for 93231.csv\n","Preprocessed and saved data for 03144.csv\n","Preprocessed and saved data for 93225.csv\n","Preprocessed and saved data for 03178.csv\n","Preprocessed and saved data for 23257.csv\n","Preprocessed and saved data for 23243.csv\n","Preprocessed and saved data for 00115.csv\n","Preprocessed and saved data for 93184.csv\n","Preprocessed and saved data for 53175.csv\n","Preprocessed and saved data for 23152.csv\n","Preprocessed and saved data for 93134.csv\n","Preprocessed and saved data for 23191.csv\n","Preprocessed and saved data for 23224.csv\n","Preprocessed and saved data for 23230.csv\n","Preprocessed and saved data for 93242.csv\n","Preprocessed and saved data for 93243.csv\n","Preprocessed and saved data for 23225.csv\n","Preprocessed and saved data for 93121.csv\n","Preprocessed and saved data for 23190.csv\n","Preprocessed and saved data for 00205.csv\n","Preprocessed and saved data for 53139.csv\n","Preprocessed and saved data for 23179.csv\n","Preprocessed and saved data for 23233.csv\n","Preprocessed and saved data for 93241.csv\n","Preprocessed and saved data for 24213.csv\n","Preprocessed and saved data for 23232.csv\n","Preprocessed and saved data for 23187.csv\n","Preprocessed and saved data for 00206.csv\n","Preprocessed and saved data for 00174.csv\n","Preprocessed and saved data for 03131.csv\n","Preprocessed and saved data for 93244.csv\n","Preprocessed and saved data for 93245.csv\n","Preprocessed and saved data for 23237.csv\n","Preprocessed and saved data for 24216.csv\n","Preprocessed and saved data for 23182.csv\n","Preprocessed and saved data for 23155.csv\n","Preprocessed and saved data for 23157.csv\n","Preprocessed and saved data for 94299.csv\n","Preprocessed and saved data for 23234.csv\n","Preprocessed and saved data for 23208.csv\n","Preprocessed and saved data for 24215.csv\n","Preprocessed and saved data for 00228.csv\n","Preprocessed and saved data for 93115.csv\n","Preprocessed and saved data for 00392.csv\n","Preprocessed and saved data for 03102.csv\n","Preprocessed and saved data for 23199.csv\n","Preprocessed and saved data for 53130.csv\n","Preprocessed and saved data for 23158.csv\n","Preprocessed and saved data for 93116.csv\n","Preprocessed and saved data for 23206.csv\n","Preprocessed and saved data for 00346.csv\n","Preprocessed and saved data for 23213.csv\n","Preprocessed and saved data for 93117.csv\n","Preprocessed and saved data for 00227.csv\n","Preprocessed and saved data for 53119.csv\n","Preprocessed and saved data for 53121.csv\n","Preprocessed and saved data for 23149.csv\n","Preprocessed and saved data for 23161.csv\n","Preprocessed and saved data for 93107.csv\n","Preprocessed and saved data for 23203.csv\n","Preprocessed and saved data for 03104.csv\n","Preprocessed and saved data for 23202.csv\n","Preprocessed and saved data for 00395.csv\n","Preprocessed and saved data for 93112.csv\n","Preprocessed and saved data for 23174.csv\n","Preprocessed and saved data for 53120.csv\n","Preprocessed and saved data for 93138.csv\n","Preprocessed and saved data for 93110.csv\n","Preprocessed and saved data for 93104.csv\n","Preprocessed and saved data for 00397.csv\n","Preprocessed and saved data for 00369.csv\n","Preprocessed and saved data for 00433.csv\n","Preprocessed and saved data for 00396.csv\n","Preprocessed and saved data for 93111.csv\n","Preprocessed and saved data for 23188.csv\n","Preprocessed and saved data for 53150.csv\n","Preprocessed and saved data for 53144.csv\n","Preprocessed and saved data for 23110.csv\n","Preprocessed and saved data for 04222.csv\n","Preprocessed and saved data for 23272.csv\n","Preprocessed and saved data for 93214.csv\n","Preprocessed and saved data for 93228.csv\n","Preprocessed and saved data for 93201.csv\n","Preprocessed and saved data for 03174.csv\n","Preprocessed and saved data for 23273.csv\n","Preprocessed and saved data for 53151.csv\n","Preprocessed and saved data for 53186.csv\n","Preprocessed and saved data for 23271.csv\n","Preprocessed and saved data for 23259.csv\n","Preprocessed and saved data for 93203.csv\n","Preprocessed and saved data for 24286.csv\n","Preprocessed and saved data for 03177.csv\n","Preprocessed and saved data for 93216.csv\n","Preprocessed and saved data for 23258.csv\n","Preprocessed and saved data for 53152.csv\n","Preprocessed and saved data for 00320.csv\n","Preprocessed and saved data for 03167.csv\n","Preprocessed and saved data for 93206.csv\n","Preprocessed and saved data for 03166.csv\n","Preprocessed and saved data for 24283.csv\n","Preprocessed and saved data for 23275.csv\n","Preprocessed and saved data for 53143.csv\n","Preprocessed and saved data for 53141.csv\n","Preprocessed and saved data for 23129.csv\n","Preprocessed and saved data for 00135.csv\n","Preprocessed and saved data for 23277.csv\n","Preprocessed and saved data for 93205.csv\n","Preprocessed and saved data for 00479.csv\n","Preprocessed and saved data for 03165.csv\n","Preprocessed and saved data for 93210.csv\n","Preprocessed and saved data for 03171.csv\n","Preprocessed and saved data for 03159.csv\n","Preprocessed and saved data for 23289.csv\n","Preprocessed and saved data for 24257.csv\n","Preprocessed and saved data for 23114.csv\n"]}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","####################################################################################\n","# Process 139 files in lcd mod to\n","# /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed\n","# we make the mixed data types a single data type aside from\n","# sky conditions and weather type which are complex strings and nan floats\n","####################################################################################\n","\n","\n","def preprocess_lcd_data(file_path):\n","    # Load the CSV with low_memory=False to prevent DtypeWarnings\n","    lcd_data = pd.read_csv(file_path, low_memory=False)\n","\n","    # List of columns with mixed types (string representations of numbers and special characters)\n","    mixed_type_cols = ['HourlyDewPointTemperature', 'HourlyDryBulbTemperature',\n","                       'HourlyPrecipitation', 'HourlyWindGustSpeed', 'HourlyWindSpeed',\n","                       'HourlyRelativeHumidity', 'HourlyWetBulbTemperature', 'HourlyWindDirection']\n","\n","    # Convert string representations to numeric and handle special characters\n","    for col in mixed_type_cols:\n","        # Remove trailing 's', convert to numeric, and replace non-numeric with NaN\n","        lcd_data[col] = pd.to_numeric(lcd_data[col].astype(str).str.rstrip('s').replace('', np.nan), errors='coerce')\n","\n","    # Handle special characters in non-numeric string columns\n","    lcd_data['HourlyPresentWeatherType'] = lcd_data['HourlyPresentWeatherType'].replace(['*', 'VRB'], np.nan)\n","    lcd_data['HourlySkyConditions'] = lcd_data['HourlySkyConditions'].replace('*', np.nan)\n","\n","    # Additional preprocessing steps if required\n","\n","    return lcd_data\n","\n","lcd_data_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod'\n","preprocessed_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed'\n","os.makedirs(preprocessed_folder, exist_ok=True)\n","\n","for file_name in os.listdir(lcd_data_folder):\n","    if file_name.endswith('.csv'):\n","        file_path = os.path.join(lcd_data_folder, file_name)\n","        preprocessed_lcd_data = preprocess_lcd_data(file_path)\n","        output_file_path = os.path.join(preprocessed_folder, file_name)\n","        preprocessed_lcd_data.to_csv(output_file_path, index=False)\n","        print(f\"Preprocessed and saved data for {file_name}\")\n"]},{"cell_type":"code","execution_count":null,"id":"341e4aba","metadata":{"id":"341e4aba","outputId":"2014afa5-9840-43b9-cbf8-ac913d95ebe1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 50/139 files in /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed\n","Processed 100/139 files in /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed\n","Processed 139/139 files in /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed\n","Processed 50/139 files in /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod\n","Processed 100/139 files in /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod\n","Processed 139/139 files in /Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod\n","Processed Folder (Preprocessed):\n","Column: STATION_ID, Data Types: {<class 'int'>}\n","Column: YEAR, Data Types: {<class 'int'>}\n","Column: MONTH, Data Types: {<class 'int'>}\n","Column: DAY, Data Types: {<class 'int'>}\n","Column: HOUR, Data Types: {<class 'int'>}\n","Column: MINUTE, Data Types: {<class 'int'>}\n","Column: HourlyDewPointTemperature, Data Types: {<class 'float'>}\n","Column: HourlyDryBulbTemperature, Data Types: {<class 'float'>}\n","Column: HourlyPrecipitation, Data Types: {<class 'float'>}\n","Column: HourlyPresentWeatherType, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyRelativeHumidity, Data Types: {<class 'float'>}\n","Column: HourlySkyConditions, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyWetBulbTemperature, Data Types: {<class 'float'>}\n","Column: HourlyWindDirection, Data Types: {<class 'float'>}\n","Column: HourlyWindGustSpeed, Data Types: {<class 'float'>}\n","Column: HourlyWindSpeed, Data Types: {<class 'float'>}\n","\n","Unprocessed Folder:\n","Column: STATION_ID, Data Types: {<class 'int'>}\n","Column: YEAR, Data Types: {<class 'int'>}\n","Column: MONTH, Data Types: {<class 'int'>}\n","Column: DAY, Data Types: {<class 'int'>}\n","Column: HOUR, Data Types: {<class 'int'>}\n","Column: MINUTE, Data Types: {<class 'int'>}\n","Column: HourlyDewPointTemperature, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyDryBulbTemperature, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyPrecipitation, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyPresentWeatherType, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyRelativeHumidity, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlySkyConditions, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyWetBulbTemperature, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyWindDirection, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyWindGustSpeed, Data Types: {<class 'str'>, <class 'float'>}\n","Column: HourlyWindSpeed, Data Types: {<class 'str'>, <class 'float'>}\n"]}],"source":["import pandas as pd\n","import os\n","from collections import defaultdict\n","\n","####################################################################################\n","# Compare the data types of each column from the pre and post processed 139 files.\n","####################################################################################\n","\n","# Directory containing the LCD data files\n","lcd_data_dir_processed = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed'\n","lcd_data_dir_unprocessed = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod'\n","\n","# Dictionary to hold data type information for both folders\n","data_types_info_processed = defaultdict(set)\n","data_types_info_unprocessed = defaultdict(set)\n","\n","# Function to process a directory and update data type information\n","def process_directory(directory, data_types_info):\n","    # List all CSV files in the directory\n","    lcd_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n","    total_files = len(lcd_files)\n","\n","    # Process each file in the directory\n","    for idx, filename in enumerate(lcd_files):\n","        file_path = os.path.join(directory, filename)\n","        try:\n","            # Read the file into a DataFrame\n","            lcd_data = pd.read_csv(file_path, low_memory=False)\n","\n","            # Update data type information for each column\n","            for col in lcd_data.columns:\n","                data_types_info[col].update(set(lcd_data[col].apply(type).unique()))\n","\n","        except Exception as e:\n","            print(f\"Error processing {filename}: {e}\")\n","\n","        # Print progress every 50 files\n","        if (idx + 1) % 50 == 0 or idx + 1 == total_files:\n","            print(f\"Processed {idx + 1}/{total_files} files in {directory}\")\n","\n","# Process both directories\n","process_directory(lcd_data_dir_processed, data_types_info_processed)\n","process_directory(lcd_data_dir_unprocessed, data_types_info_unprocessed)\n","\n","# Display the aggregated data type information for both folders\n","print(\"Processed Folder (Preprocessed):\")\n","for col, types in data_types_info_processed.items():\n","    print(f\"Column: {col}, Data Types: {types}\")\n","\n","print(\"\\nUnprocessed Folder:\")\n","for col, types in data_types_info_unprocessed.items():\n","    print(f\"Column: {col}, Data Types: {types}\")\n"]},{"cell_type":"code","execution_count":null,"id":"b6fd026b","metadata":{"id":"b6fd026b"},"outputs":[],"source":["shared_features = {\n","    'Precipitation': {\n","        'lcd_col': 'HourlyPrecipitation',\n","        'ghcn_col': 'PRCP',\n","        'processing': 'sum'  # Sum of hourly precipitation for daily total\n","    },\n","    'Temperature': {\n","        'lcd_col': 'HourlyDryBulbTemperature',\n","        'ghcn_cols': ['TMAX', 'TMIN', 'TAVG'],\n","        'processing': ['max', 'min', 'mean']  # Max, Min, and Average of hourly temperatures\n","    },\n","    'DewPointTemperature': {\n","        'lcd_col': 'HourlyDewPointTemperature',\n","        'ghcn_col': 'ADPT',\n","        'processing': 'mean'  # Average of hourly dew point temperatures\n","    },\n","    'WetBulbTemperature': {\n","        'lcd_col': 'HourlyWetBulbTemperature',\n","        'ghcn_col': 'AWBT',\n","        'processing': 'mean'  # Average of hourly wet bulb temperatures\n","    },\n","    'WindSpeed': {\n","        'lcd_col': 'HourlyWindSpeed',\n","        'ghcn_col': 'AWND',\n","        'processing': 'mean'  # Average of hourly wind speeds\n","    },\n","    'RelativeHumidity': {\n","        'lcd_col': 'HourlyRelativeHumidity',\n","        'ghcn_cols': ['RHAV', 'RHMN', 'RHMX'],\n","        'processing': ['mean', 'min', 'max']  # Average, Minimum, and Maximum of hourly relative humidity\n","    }\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"f63d4e48","metadata":{"id":"f63d4e48"},"outputs":[],"source":["# Each feature's daily value is directly used as it is already in a daily format.\n","unique_ghcn_features = ['SNOW', 'SNWD', 'EVAP', 'FMTM', 'FRGB', 'FRGT', 'FRTH']\n","\n","\n","unique_lcd_features = {\n","    'MaxWindSpeed': {\n","        'lcd_col': 'HourlyWindSpeed',\n","        'processing': 'max'  # Maximum of hourly wind speeds\n","    },\n","    'WindDirectionMode': {\n","        'lcd_col': 'HourlyWindDirection',\n","        'processing': 'mode'  # Most common wind direction\n","    },\n","    'WindDirectionAverage': {\n","        'lcd_col': 'HourlyWindDirection',\n","        'processing': 'mean'  # Average of hourly wind directions (with circular data handling)\n","    },\n","    'MaxWindGust': {\n","        'lcd_col': 'HourlyWindGustSpeed',\n","        'processing': 'max'  # Maximum of hourly wind gust speeds\n","    },\n","    'WeatherTypeMode': {\n","        'lcd_col': 'HourlyPresentWeatherType',\n","        'processing': 'mode'  # Most common weather type\n","    },\n","    'SkyConditionMode': {\n","        'lcd_col': 'HourlySkyConditions',\n","        'processing': 'mode'  # Most common sky condition\n","    }\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"a84e3f98","metadata":{"id":"a84e3f98","outputId":"56274850-712a-4df5-a040-7803670ea110"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 23131.csv\n","Processed and saved daily data for 23119.csv\n","Processed and saved daily data for 03183.csv\n","Processed and saved daily data for 03154.csv\n","Processed and saved daily data for 93209.csv\n","Processed and saved daily data for 23285.csv\n","Processed and saved daily data for 03182.csv\n","Processed and saved daily data for 23130.csv\n","Processed and saved daily data for 93197.csv\n","Processed and saved daily data for 24259.csv\n","Processed and saved daily data for 03180.csv\n","Processed and saved daily data for 23244.csv\n","Processed and saved daily data for 23293.csv\n","Processed and saved daily data for 03181.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 00117.csv\n","Processed and saved daily data for 93227.csv\n","Processed and saved daily data for 93232.csv\n","Processed and saved daily data for 23254.csv\n","Processed and saved daily data for 93144.csv\n","Processed and saved daily data for 23136.csv\n","Processed and saved daily data for 93193.csv\n","Processed and saved daily data for 23122.csv\n","Processed and saved daily data for 03179.csv\n","Processed and saved daily data for 93230.csv\n","Processed and saved daily data for 93231.csv\n","Processed and saved daily data for 03144.csv\n","Processed and saved daily data for 93225.csv\n","Processed and saved daily data for 03178.csv\n","Processed and saved daily data for 23257.csv\n","Processed and saved daily data for 23243.csv\n","Processed and saved daily data for 00115.csv\n","Processed and saved daily data for 93184.csv\n","Processed and saved daily data for 53175.csv\n","Processed and saved daily data for 23152.csv\n","Processed and saved daily data for 93134.csv\n","Processed and saved daily data for 23191.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 23224.csv\n","Processed and saved daily data for 23230.csv\n","Processed and saved daily data for 93242.csv\n","Processed and saved daily data for 93243.csv\n","Processed and saved daily data for 23225.csv\n","Processed and saved daily data for 93121.csv\n","Processed and saved daily data for 23190.csv\n","Processed and saved daily data for 00205.csv\n","Processed and saved daily data for 53139.csv\n","Processed and saved daily data for 23179.csv\n","Processed and saved daily data for 23233.csv\n","Processed and saved daily data for 93241.csv\n","Processed and saved daily data for 24213.csv\n","Processed and saved daily data for 23232.csv\n","Processed and saved daily data for 23187.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 00206.csv\n","Processed and saved daily data for 00174.csv\n","Processed and saved daily data for 03131.csv\n","Processed and saved daily data for 93244.csv\n","Processed and saved daily data for 93245.csv\n","Processed and saved daily data for 23237.csv\n","Processed and saved daily data for 24216.csv\n","Processed and saved daily data for 23182.csv\n","Processed and saved daily data for 23155.csv\n","Processed and saved daily data for 23157.csv\n","Processed and saved daily data for 94299.csv\n","Processed and saved daily data for 23234.csv\n","Processed and saved daily data for 23208.csv\n","Processed and saved daily data for 24215.csv\n","Processed and saved daily data for 00228.csv\n","Processed and saved daily data for 93115.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 00392.csv\n","Processed and saved daily data for 03102.csv\n","Processed and saved daily data for 23199.csv\n","Processed and saved daily data for 53130.csv\n","Processed and saved daily data for 23158.csv\n","Processed and saved daily data for 93116.csv\n","Processed and saved daily data for 23206.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 00346.csv\n","Processed and saved daily data for 23213.csv\n","Processed and saved daily data for 93117.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 00227.csv\n","Processed and saved daily data for 53119.csv\n","Processed and saved daily data for 53121.csv\n","Processed and saved daily data for 23149.csv\n","Processed and saved daily data for 23161.csv\n","Processed and saved daily data for 93107.csv\n","Processed and saved daily data for 23203.csv\n","Processed and saved daily data for 03104.csv\n","Processed and saved daily data for 23202.csv\n","Processed and saved daily data for 00395.csv\n","Processed and saved daily data for 93112.csv\n","Processed and saved daily data for 23174.csv\n","Processed and saved daily data for 53120.csv\n","Processed and saved daily data for 93138.csv\n","Processed and saved daily data for 93110.csv\n","Processed and saved daily data for 93104.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 00397.csv\n","Processed and saved daily data for 00369.csv\n","Processed and saved daily data for 00433.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9b/w58m9pbs7dsdp2l04rxrznlw0000gn/T/ipykernel_17902/246626639.py:59: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  lcd_data = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Processed and saved daily data for 00396.csv\n","Processed and saved daily data for 93111.csv\n","Processed and saved daily data for 23188.csv\n","Processed and saved daily data for 53150.csv\n","Processed and saved daily data for 53144.csv\n","Processed and saved daily data for 23110.csv\n","Processed and saved daily data for 04222.csv\n","Processed and saved daily data for 23272.csv\n","Processed and saved daily data for 93214.csv\n","Processed and saved daily data for 93228.csv\n","Processed and saved daily data for 93201.csv\n","Processed and saved daily data for 03174.csv\n","Processed and saved daily data for 23273.csv\n","Processed and saved daily data for 53151.csv\n","Processed and saved daily data for 53186.csv\n","Processed and saved daily data for 23271.csv\n","Processed and saved daily data for 23259.csv\n","Processed and saved daily data for 93203.csv\n","Processed and saved daily data for 24286.csv\n","Processed and saved daily data for 03177.csv\n","Processed and saved daily data for 93216.csv\n","Processed and saved daily data for 23258.csv\n","Processed and saved daily data for 53152.csv\n","Processed and saved daily data for 00320.csv\n","Processed and saved daily data for 03167.csv\n","Processed and saved daily data for 93206.csv\n","Processed and saved daily data for 03166.csv\n","Processed and saved daily data for 24283.csv\n","Processed and saved daily data for 23275.csv\n","Processed and saved daily data for 53143.csv\n","Processed and saved daily data for 53141.csv\n","Processed and saved daily data for 23129.csv\n","Processed and saved daily data for 00135.csv\n","Processed and saved daily data for 23277.csv\n","Processed and saved daily data for 93205.csv\n","Processed and saved daily data for 00479.csv\n","Processed and saved daily data for 03165.csv\n","Processed and saved daily data for 93210.csv\n","Processed and saved daily data for 03171.csv\n","Processed and saved daily data for 03159.csv\n","Processed and saved daily data for 23289.csv\n","Processed and saved daily data for 24257.csv\n","Processed and saved daily data for 23114.csv\n"]}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","####################################################################################\n","# Convert the 139 post processed files into a new converted folder that converts\n","# the hourly measurments to daily min, max, averages and modes.\n","####################################################################################\n","\n","def calculate_circular_mean(angles):\n","    # Remove NaN values from angles\n","    angles = angles.dropna()\n","\n","    # If no valid angles, return NaN\n","    if len(angles) == 0:\n","        return np.nan\n","\n","    # Convert angles from degrees to radians\n","    angles_rad = np.radians(angles)\n","\n","    # Calculate sine and cosine components\n","    sin_components = np.sin(angles_rad)\n","    cos_components = np.cos(angles_rad)\n","\n","    # Calculate mean vector components\n","    avg_sin = np.mean(sin_components)\n","    avg_cos = np.mean(cos_components)\n","\n","    # Calculate the average wind direction from the mean vector components\n","    avg_wind_direction = np.degrees(np.arctan2(avg_sin, avg_cos))\n","\n","    # Adjust the range from -180° to 180° to 0° to 360°\n","    if avg_wind_direction < 0:\n","        avg_wind_direction += 360\n","\n","    return avg_wind_direction\n","\n","def mode_function(series):\n","    mode_result = series.mode()\n","    return mode_result.iloc[0] if not mode_result.empty else np.nan\n","\n","def custom_dew_point_mean(series):\n","    DEW_POINT_TEMP_RANGE = (-22, 86)  # in °F\n","    filtered_series = series[(series >= DEW_POINT_TEMP_RANGE[0]) & (series <= DEW_POINT_TEMP_RANGE[1])]\n","    return filtered_series.mean()\n","\n","def custom_wind_speed_mean(series):\n","    WIND_SPEED_RANGE = (0, 200)  # in mph, using your provided range\n","    filtered_series = series[(series >= WIND_SPEED_RANGE[0]) & (series <= WIND_SPEED_RANGE[1])]\n","    return filtered_series.mean()\n","\n","def custom_max(series):\n","    WIND_SPEED_RANGE = (0, 200)  # in mph\n","    filtered_series = series[(series >= WIND_SPEED_RANGE[0]) & (series <= WIND_SPEED_RANGE[1])]\n","    return filtered_series.max()\n","\n","\n","def process_lcd_file(file_path):\n","    lcd_data = pd.read_csv(file_path)\n","    lcd_data['HOUR'] = pd.to_datetime(lcd_data['HOUR'], format='%H').dt.hour\n","\n","    agg_functions = {\n","        'HourlyPrecipitation': 'sum',\n","        'HourlyDryBulbTemperature': ['max', 'min', 'mean'],\n","        'HourlyDewPointTemperature': custom_dew_point_mean,\n","        'HourlyWetBulbTemperature': 'mean',\n","        'HourlyWindSpeed': [custom_max, custom_wind_speed_mean],\n","        'HourlyRelativeHumidity': ['max', 'min', 'mean'],\n","        'HourlyWindDirection': [calculate_circular_mean, mode_function],\n","        'HourlyWindGustSpeed': 'max',\n","        'HourlyPresentWeatherType': mode_function,\n","        'HourlySkyConditions': mode_function\n","    }\n","\n","    daily_data = lcd_data.groupby(['YEAR', 'MONTH', 'DAY']).agg(agg_functions).reset_index()\n","\n","\n","    # Flatten MultiIndex columns and rename\n","    new_columns = []\n","    for col in daily_data.columns:\n","        if isinstance(col, tuple):\n","            # Prefix the aggregation function name and append the rest of the column name (without 'Hourly')\n","            new_column_name = col[1] + col[0].replace('Hourly', '')\n","            new_columns.append(new_column_name)\n","        else:\n","            new_columns.append(col)\n","    daily_data.columns = new_columns\n","\n","    daily_data = daily_data.rename(columns={\n","        'custom_dew_point_meanDewPointTemperature': 'meanDewPointTemperature',\n","        'custom_maxWindSpeed': 'maxWindSpeed',\n","        'custom_wind_speed_meanWindSpeed': 'meanWindSpeed'\n","    })\n","\n","    return daily_data\n","\n","lcd_data_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed'\n","processed_converted_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed_converted'\n","os.makedirs(processed_converted_folder, exist_ok=True)\n","\n","for file_name in os.listdir(lcd_data_folder):\n","    if file_name.endswith('.csv'):\n","        file_path = os.path.join(lcd_data_folder, file_name)\n","        daily_lcd_data = process_lcd_file(file_path)\n","        output_file_path = os.path.join(processed_converted_folder, file_name)\n","        daily_lcd_data.to_csv(output_file_path, index=False)\n","        print(f\"Processed and saved daily data for {file_name}\")\n"]},{"cell_type":"code","execution_count":null,"id":"dd2c8399","metadata":{"id":"dd2c8399","outputId":"6d940047-08e1-40cd-fc4e-d7ba5dac1eb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Manually Aggregated Columns:\n","['YEAR', 'MONTH', 'DAY', 'sumPrecipitation', 'maxDryBulbTemperature', 'minDryBulbTemperature', 'meanDryBulbTemperature', 'meanDewPointTemperature', 'meanWetBulbTemperature', 'maxWindSpeed', 'meanWindSpeed', 'maxRelativeHumidity', 'minRelativeHumidity', 'meanRelativeHumidity', 'calculate_circular_meanWindDirection', 'maxWindGustSpeed', 'mode_functionPresentWeatherType', 'mode_functionSkyConditions']\n","\n","Daily CSV Columns:\n","['YEAR', 'MONTH', 'DAY', 'sumPrecipitation', 'maxDryBulbTemperature', 'minDryBulbTemperature', 'meanDryBulbTemperature', 'meanDewPointTemperature', 'meanWetBulbTemperature', 'maxWindSpeed', 'meanWindSpeed', 'maxRelativeHumidity', 'minRelativeHumidity', 'meanRelativeHumidity', 'calculate_circular_meanWindDirection', 'maxWindGustSpeed', 'mode_functionPresentWeatherType', 'mode_functionSkyConditions']\n","\n","Comparing for file: 00115.csv on 2023-11-3\n","YEAR: 2023 | 2023\n","MONTH: 11 | 11\n","DAY: 3 | 3\n","sumPrecipitation: 0.0 | 0.0\n","maxDryBulbTemperature: 68.0 | 68.0\n","minDryBulbTemperature: 25.0 | 25.0\n","meanDryBulbTemperature: 42.42253521126761 | 42.42253521126761\n","meanDewPointTemperature: 6.929577464788732 | 6.929577464788732\n","meanWetBulbTemperature: 29.408450704225352 | 29.408450704225352\n","maxWindSpeed: 9.0 | 9.0\n","meanWindSpeed: 1.8169014084507042 | 1.816901408450704\n","maxRelativeHumidity: 44.0 | 44.0\n","minRelativeHumidity: 6.0 | 6.0\n","meanRelativeHumidity: 27.661971830985916 | 27.66197183098592\n","calculate_circular_meanWindDirection: 16.18303625364795 | 16.18303625364795\n","maxWindGustSpeed: nan | nan\n","mode_functionPresentWeatherType: nan | nan\n","mode_functionSkyConditions: CLR:00 | CLR:00\n"]}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","####################################################################################\n","# This is just a script that compares the converted daily interval files to the pre converted\n","# hourly/minute interval files\n","####################################################################################\n","\n","def calculate_circular_mean(angles):\n","    angles = angles.dropna()\n","    if len(angles) == 0:\n","        return np.nan\n","    angles_rad = np.radians(angles)\n","    avg_sin = np.mean(np.sin(angles_rad))\n","    avg_cos = np.mean(np.cos(angles_rad))\n","    avg_wind_direction = np.degrees(np.arctan2(avg_sin, avg_cos))\n","    return avg_wind_direction if avg_wind_direction >= 0 else avg_wind_direction + 360\n","\n","def mode_function(series):\n","    mode_result = series.mode()\n","    return mode_result.iloc[0] if not mode_result.empty else np.nan\n","\n","def compare_hourly_daily(file_name, hourly_folder, daily_folder, year, month, day):\n","    # Load hourly data\n","    hourly_data = pd.read_csv(os.path.join(hourly_folder, file_name))\n","    # Load daily data\n","    daily_data = pd.read_csv(os.path.join(daily_folder, file_name))\n","\n","    # Filter data for the specified year, month, and day\n","    hourly_data_specific_day = hourly_data[(hourly_data['YEAR'] == year) & (hourly_data['MONTH'] == month) & (hourly_data['DAY'] == day)]\n","    daily_data_specific_day = daily_data[(daily_data['YEAR'] == year) & (daily_data['MONTH'] == month) & (daily_data['DAY'] == day)]\n","\n","    # Define aggregation functions\n","    agg_functions = {\n","        'HourlyPrecipitation': 'sum',\n","        'HourlyDryBulbTemperature': ['max', 'min', 'mean'],\n","        'HourlyDewPointTemperature': 'mean',\n","        'HourlyWetBulbTemperature': 'mean',\n","        'HourlyWindSpeed': ['max', 'mean'],\n","        'HourlyRelativeHumidity': ['max', 'min', 'mean'],\n","        'HourlyWindDirection': calculate_circular_mean,\n","        'HourlyWindGustSpeed': 'max',\n","        'HourlyPresentWeatherType': mode_function,\n","        'HourlySkyConditions': mode_function\n","    }\n","\n","    # Perform aggregation\n","    daily_aggregated_specific_day = hourly_data_specific_day.groupby(['YEAR', 'MONTH', 'DAY']).agg(agg_functions).reset_index()\n","\n","    # Rename columns in daily_aggregated_specific_day to match daily_data_specific_day\n","    daily_aggregated_specific_day.columns = [\n","        'YEAR', 'MONTH', 'DAY', 'sumPrecipitation', 'maxDryBulbTemperature',\n","        'minDryBulbTemperature', 'meanDryBulbTemperature', 'meanDewPointTemperature',\n","        'meanWetBulbTemperature', 'maxWindSpeed', 'meanWindSpeed', 'maxRelativeHumidity',\n","        'minRelativeHumidity', 'meanRelativeHumidity', 'calculate_circular_meanWindDirection',\n","        'maxWindGustSpeed', 'mode_functionPresentWeatherType', 'mode_functionSkyConditions'\n","    ]\n","\n","    # Print column names for debugging\n","    print(\"Manually Aggregated Columns:\")\n","    print(daily_aggregated_specific_day.columns.to_list())\n","    print(\"\\nDaily CSV Columns:\")\n","    print(daily_data_specific_day.columns.to_list())\n","\n","    # Compare the values for the specific day\n","    print(f\"\\nComparing for file: {file_name} on {year}-{month}-{day}\")\n","    for col in daily_aggregated_specific_day.columns:\n","        if col in daily_data_specific_day.columns:\n","            print(f\"{col}: {daily_aggregated_specific_day[col].values[0]} | {daily_data_specific_day[col].values[0]}\")\n","\n","hourly_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed'\n","daily_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed_converted'\n","compare_hourly_daily('00115.csv', hourly_folder, daily_folder, year=2023, month=11, day=3)\n"]},{"cell_type":"code","execution_count":null,"id":"c387795e","metadata":{"id":"c387795e","outputId":"64fb515c-219e-4de2-fa00-6bd1977b74e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Manually Aggregated Columns:\n","['YEAR', 'MONTH', 'DAY', 'sumPrecipitation', 'maxDryBulbTemperature', 'minDryBulbTemperature', 'meanDryBulbTemperature', 'meanDewPointTemperature', 'meanWetBulbTemperature', 'maxWindSpeed', 'meanWindSpeed', 'maxRelativeHumidity', 'minRelativeHumidity', 'meanRelativeHumidity', 'calculate_circular_meanWindDirection', 'mode_functionWindDirection', 'maxWindGustSpeed', 'mode_functionPresentWeatherType', 'mode_functionSkyConditions']\n","\n","Daily CSV Columns:\n","['YEAR', 'MONTH', 'DAY', 'sumPrecipitation', 'maxDryBulbTemperature', 'minDryBulbTemperature', 'meanDryBulbTemperature', 'meanDewPointTemperature', 'meanWetBulbTemperature', 'maxWindSpeed', 'meanWindSpeed', 'maxRelativeHumidity', 'minRelativeHumidity', 'meanRelativeHumidity', 'calculate_circular_meanWindDirection', 'mode_functionWindDirection', 'maxWindGustSpeed', 'mode_functionPresentWeatherType', 'mode_functionSkyConditions']\n","\n","Comparing for file: 00115.csv on 2023-11-3\n","YEAR: 2023 | 2023\n","MONTH: 11 | 11\n","DAY: 3 | 3\n","sumPrecipitation: 0.0 | 0.0\n","maxDryBulbTemperature: 68.0 | 68.0\n","minDryBulbTemperature: 25.0 | 25.0\n","meanDryBulbTemperature: 42.42253521126761 | 42.42253521126761\n","meanDewPointTemperature: 6.929577464788732 | 6.929577464788732\n","meanWetBulbTemperature: 29.408450704225352 | 29.408450704225352\n","maxWindSpeed: 9.0 | 9.0\n","meanWindSpeed: 1.8169014084507042 | 1.816901408450704\n","maxRelativeHumidity: 44.0 | 44.0\n","minRelativeHumidity: 6.0 | 6.0\n","meanRelativeHumidity: 27.661971830985916 | 27.66197183098592\n","calculate_circular_meanWindDirection: 16.18303625364795 | 16.18303625364795\n","mode_functionWindDirection: 0.0 | 0.0\n","maxWindGustSpeed: nan | nan\n","mode_functionPresentWeatherType: nan | nan\n","mode_functionSkyConditions: CLR:00 | CLR:00\n"]}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","\n","####################################################################################\n","# This is just a script that compares the converted daily interval files to the pre converted\n","# hourly/minute interval files\n","####################################################################################\n","\n","def calculate_circular_mean(angles):\n","    angles = angles.dropna()\n","    if len(angles) == 0:\n","        return np.nan\n","    angles_rad = np.radians(angles)\n","    avg_sin = np.mean(np.sin(angles_rad))\n","    avg_cos = np.mean(np.cos(angles_rad))\n","    avg_wind_direction = np.degrees(np.arctan2(avg_sin, avg_cos))\n","    return avg_wind_direction if avg_wind_direction >= 0 else avg_wind_direction + 360\n","\n","def mode_function(series):\n","    mode_result = series.mode()\n","    return mode_result.iloc[0] if not mode_result.empty else np.nan\n","\n","def compare_hourly_daily(file_name, hourly_folder, daily_folder, year, month, day):\n","    # Load hourly data\n","    hourly_data = pd.read_csv(os.path.join(hourly_folder, file_name))\n","    # Load daily data\n","    daily_data = pd.read_csv(os.path.join(daily_folder, file_name))\n","\n","    # Filter data for the specified year, month, and day\n","    hourly_data_specific_day = hourly_data[(hourly_data['YEAR'] == year) & (hourly_data['MONTH'] == month) & (hourly_data['DAY'] == day)]\n","    daily_data_specific_day = daily_data[(daily_data['YEAR'] == year) & (daily_data['MONTH'] == month) & (daily_data['DAY'] == day)]\n","\n","    # Define aggregation functions\n","    agg_functions = {\n","        'HourlyPrecipitation': 'sum',\n","        'HourlyDryBulbTemperature': ['max', 'min', 'mean'],\n","        'HourlyDewPointTemperature': 'mean',\n","        'HourlyWetBulbTemperature': 'mean',\n","        'HourlyWindSpeed': ['max', 'mean'],\n","        'HourlyRelativeHumidity': ['max', 'min', 'mean'],\n","        'HourlyWindDirection': [calculate_circular_mean, mode_function],  # Added mode calculation\n","        'HourlyWindGustSpeed': 'max',\n","        'HourlyPresentWeatherType': mode_function,\n","        'HourlySkyConditions': mode_function\n","    }\n","\n","    # Perform aggregation\n","    daily_aggregated_specific_day = hourly_data_specific_day.groupby(['YEAR', 'MONTH', 'DAY']).agg(agg_functions).reset_index()\n","\n","    # Flatten MultiIndex columns and rename to match daily_data_specific_day\n","    new_columns = []\n","    for col in daily_aggregated_specific_day.columns:\n","        if isinstance(col, tuple):\n","            new_column_name = col[1] + col[0].replace('Hourly', '')\n","            new_columns.append(new_column_name)\n","        else:\n","            new_columns.append(col)\n","    daily_aggregated_specific_day.columns = new_columns\n","\n","    # Print column names for debugging\n","    print(\"Manually Aggregated Columns:\")\n","    print(daily_aggregated_specific_day.columns.to_list())\n","    print(\"\\nDaily CSV Columns:\")\n","    print(daily_data_specific_day.columns.to_list())\n","\n","    # Compare the values for the specific day\n","    print(f\"\\nComparing for file: {file_name} on {year}-{month}-{day}\")\n","    for col in daily_aggregated_specific_day.columns:\n","        if col in daily_data_specific_day.columns:\n","            print(f\"{col}: {daily_aggregated_specific_day[col].values[0]} | {daily_data_specific_day[col].values[0]}\")\n","\n","hourly_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed'\n","daily_folder = '/Users/Elliot/Documents/Northeastern/6140/Project/LCD_DATA/Raw_data_download_mod_preprocessed_converted'\n","compare_hourly_daily('00115.csv', hourly_folder, daily_folder, year=2023, month=11, day=3)\n"]},{"cell_type":"code","execution_count":null,"id":"f3021618","metadata":{"id":"f3021618"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"collapsed_sections":["fyEq9-Zv0Wnh","fc4c4c17","824b0ca3","d5640997"]}},"nbformat":4,"nbformat_minor":5}